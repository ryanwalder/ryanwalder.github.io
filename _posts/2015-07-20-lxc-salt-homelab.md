---
layout: post
title: 'Setting up a Homelab for use with Salt Cloud. Part 1: Decisions and Setup'
permalink: saltcloud-homelab
comments: true
---

After running a handful of VMs on my main workstation for labs for many years the frustrations of having to manually provision, manage, and shut them down when needing to do other intensive tasks finally got too much and I decided I wanted to build a machine I could use to set up a small virtualization server/private cloud with one which I could later expand if needed, most importantly one which I could control with [salt-cloud](https://docs.saltstack.com/en/develop/topics/cloud/index.html) for automated creation, management, and destruction.

# Hardware

Most of the time any given VMs will likely be sat there idle or only require minimal resources so I wanted something I could squeeze a lot out of resource wise, I didn't really need anything fancy and running full racks of older underspecced refurbished systems at home doesn't really appeal to me (I don't have room for a rack in my flat, and electricity is expensive!) so I decided for a simple whitebox build with plenty of RAM and a CPU that's half decent, I settled on the build below:

Type|Item|Price
:----|:----|:----
**CPU** | [AMD FX-9590 4.7GHz 8-Core OEM/Tray Processor](http://uk.pcpartpicker.com/part/amd-cpu-fd9590fhhkwof) | £161.94 @ Aria PC
**CPU Cooler** | [Corsair H100i 77.0 CFM Liquid CPU Cooler](http://uk.pcpartpicker.com/part/corsair-cpu-cooler-h100i) | £93.72 @ Scan.co.uk
**Motherboard** | [Gigabyte GA-990FXA-UD3 ATX AM3+ Motherboard](http://uk.pcpartpicker.com/part/gigabyte-motherboard-ga990fxaud3) | £95.99 @ Amazon UK
**Memory** | [Corsair Vengeance 32GB (4 x 8GB) DDR3-1600 Memory](http://uk.pcpartpicker.com/part/corsair-memory-cmz32gx3m4x1600c10) | £156.76 @ Ebuyer
**Storage** | [Kingston SSDNow 200 30GB 2.5" Solid State Drive](http://uk.pcpartpicker.com/part/kingston-internal-hard-drive-ss200s330g) | £24.12 @ Amazon UK
**Storage** | [Kingston SSDNow V300 Series 480GB 2.5" Solid State Drive](http://uk.pcpartpicker.com/part/kingston-internal-hard-drive-sv300s37a480g) | £119.99 @ Novatech
**Case** | [Fractal Design Define S ATX Mid Tower Case](http://uk.pcpartpicker.com/part/fractal-design-case-fdcadefsbk) | £61.33 @ CCL Computers
**Power Supply** | [Corsair CX 430W 80+ Bronze Certified Semi-Modular ATX Power Supply](http://uk.pcpartpicker.com/part/corsair-power-supply-cx430m) | £40.09 @ Amazon UK
 | **Total** | **£753.94**
 | Generated by [PCPartPicker](http://uk.pcpartpicker.com/p/GxmTgs) 2015-07-12 22:15 BST+0100 |

So as you can see, plenty of RAM and a CPU with a decent clockspeed and plenty of cores, I did consider an i7 but the price/performance ratio was too far off for a homelab box. I could have got everything a bit cheaper with a different case/PSU/cooler but I'm a sucker for Fractal cases, the cooler is complete overkill but after many years dealing with massive heatsinks AIO watercooling solution are a pleasure to install, and you don't cheap out on PSUs, I've always had good experiences with Corsair PSUs in the past so went with what I know.

For storage the 30gb drive would be for the OS (and general OS image/template/ISO storage) and the 480gb for the VM drives/filesystems, for mass storage I have a NAS if I need to offload/store any huge files, most machines are not likely to go above ~10-50gb for their root file system so this should be plenty.

# "Virtualization" Options

I took a look around to see what options were out there and came up with the following list:

1. [OpenStack](https://www.openstack.org/)
2. [CloudStack](https://cloudstack.apache.org/)
3. [ESXi](http://www.vmware.com/products/esxi-and-esx/overview)
4. [ProxMox](http://www.proxmox.com/en/)
5. [oVirt](http://www.ovirt.org/)
6. [OpenNode](http://opennodecloud.com/)
7. [Archipel](http://archipelproject.org/)
8. [LXC](https://github.com/lxc/lxc)
9. [Docker](https://www.docker.com/)

OpenStack and CloudStack were basically a no go because for a production(ish) deployment you need a whole mess of machines to even get a basic setup going. ESXi, ProxMox, oVirt, and OpenNode all looked promising giving good remote management options and a few being able to integrate with Salt Cloud, Archipel was interesting in that management was done over XMPP but I ultimately decided against using full VMs in favour of containers as I'll only be running Linux machines and could get a higher density of machines running without the overhead of virtual machines that leaves me with LXC and Docker and as I'm more interested in full systems rather than app containers that leaves LXC for my platform.

LXC gives me the ability for high machine density with very few wasted resources and some cool options for machine storage with different backend storage mechanisms (lvm, zfs, btrfs), it's also supported natively supported by Salt Cloud and gives me room to expand in the future but simply adding another LXC host.

# Setting up LXC

I decided to use Ubuntu 14.04 as my container host as it supports LXC out of the box and is generally my goto distro at the moment.

## Installation

First off, make sure the system is up to date:

`sudo apt-get update`  
`sudo apt-get upgrade -y`  
`sudo apt-get dist-upgrade -y`  

Now just need to install everything we need for LXC:

`sudo apt-get install -y lxc lxc-templates bridge-utils`

## Networking

As I want my containers accessible from outside the host and for them to get a DHCP lease I need to setup a bridge device that will route out of the machine over a physical device, for this I created the br0 device with a static IP:

`sudo vi /etc/networking/interfaces`

    auto lo
    iface lo inet loopback

    auto br0
    iface br0 inet static
      bridge_ports eth0
      bridge_fd 0
      address 192.168.1.151
      netmask 255.255.255.0
      network 192.168.1.0
      broadcast 192.168.1.255
      gateway 192.168.1.1

NOTE: You need to make sure you get rid of the default `eth0` device or things will not work!

Now to tell lxc to use br0 as the default bridge:

`sudo vi /etc/lxc/default.conf`

    lxc.network.type = veth
    lxc.network.link = br0
    lxc.network.flags = up
    lxc.network.hwaddr = 00:16:3e:xx:xx:xx

Now you'll need to apply the network config (which you'll likely not be able to do if you're ssh'd in) or reboot for it to activate:

`sudo service networking restart`  
or  
`sudo reboot`  


# ZFS

As I'm using containers I wanted to use ZFS as my backing store for the filesystems so I could take advantage of compression and de-duplication. 

## Installation

To make sure I was running a recent version of ZFS I installed from the [ZFS on Linux PPA](https://launchpad.net/~zfs-native/+archive/ubuntu/stable):


`sudo apt-add-repository ppa:zfs-native/stable`  
`sudo apt-get update`  
`sudo apt-get install ubuntu-zfs`  

## Pool creation

Once ZFS was installed I just needed to setup a pool to act as container storage:

`sudo zpool create -f lxcpool /dev/sdb`  
`sudo zfs set dedup=on lxcpool`  
`sudo zfs set compression=on lxcpool`  
`sudo zpool set listsnapshots=on lxcpool`  
`sudo zfs create lxcpool/lxc`  
`sudo zfs create lxcpool/lxc/containers`  

And add it to the default lxc config `/etc/lxc/lxc.conf`:

    lxc.lxcpath = /lxcpool/lxc/containers
    lxc.bdev.zfs.root = lxcpool/lxc/containers

# The first container

Now the setup was complete I just needed to create my first container:

`sudo lxc-create --template ubuntu --name test1`

Unfortunately I ran into a few issues when trying to download the ubuntu template where it would sporadically be unable to download packages causing the entire creation process to fail, after a bit of digging about I discovered that this was due to wget timing out when resolving `archive.ubuntu.com`, because this is a load balanced endpoint you'll end up bounced around a whole load of actual servers and one of them was timing out, to fix this I added `91.189.91.14 archive.ubuntu.com` to my `/etc/hosts` file and started the creation process again. Thankfully once you have the template successfully downloaded once any further containers created from then will use the local copy (stored in `/var/cache/lxc/<templatename>`).

Now the container is downloaded you can see it's been created:

`sudo lxc-ls --fancy`

    NAME   STATE    IPV4  IPV6  AUTOSTART
    -------------------------------------
    test1  STOPPED  -     -     NO

`ls -al /lxcpool/lxc/containers/`

    total 3
    drwxr-xr-x 3 root root    4 Jul 12 23:39 .
    drwxr-xr-x 3 root root    3 Jul 12 16:16 ..
    drwxrwx--- 3 root root    5 Jul 12 23:39 test1

`sudo zpool list`

    NAME      SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
    lxcpool   444G   177M   444G         -     0%     0%  1.01x  ONLINE  -

As you can see I now have a created container running on the zfs backend so time to start it:

`sudo lxc-start --name test1 --daemon`

This starts the container in the background and should get a DHCP lease which as you can see is now up and running and accessible from any machine on the network:

`sudo lxc-ls --fancy`

    NAME   STATE    IPV4           IPV6  AUTOSTART
    ----------------------------------------------
    test1  RUNNING  192.168.1.122  -     NO

# The second container

Now the first container is up and running I can very quickly setup a second container to have a quck look at the ZFS compression:

`sudo lxc-create -t ubuntu -n test2`  
`sudo lxc-start -n test2 -d`  
`sudo lxc-ls -f`  

    NAME   STATE    IPV4           IPV6  AUTOSTART
    ----------------------------------------------
    test1  RUNNING  192.168.1.122  -     NO
    test2  RUNNING  192.168.1.123  -     NO

Cool, now lets check the ZFS dedupe:

`sudo zpool list`

    NAME      SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
    lxcpool   444G   183M   444G         -     0%     0%  2.02x  ONLINE  -

Excellent, as you can see the `DEDUP` has gone from `1.01x` with one container to `2.02x` with two, as things go on this number will of course drop as the containers diverge in contents but when you add up the core systems for all the containers I'll be running in the long run it's a no brainer to use ZFS as the backing store as I'll save lots of space!

# Template housekeeping

The one issue with the templates is that they will age, meaning that if you download one now then don't use it for two months it'll be 2 months out of date the next time you use it, meaning you need to do an `apt-get upgrade && apt-get update` every time you start a new container which is no fun. To fix this what I like to do is setup a cronjob for the root user to jump in the chroot of the template and update it every day:

    0 0 * * * * chroot /var/cache/lxc/trusty/rootfs-amd64/ && apt-get update && apt-get dist-upgrade -y && apt-get clean

# Almost ready for Salt

Now the system is almost ready to act as a LXC host for Salt, we just need to setup the server as a Salt minion. For that though we would ideally be doing all the above with some Salt states so in the next part I'll cover getting all of this into some Salt states and having Salt provision the system from scratch.

